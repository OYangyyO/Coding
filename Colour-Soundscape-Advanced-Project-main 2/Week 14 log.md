# Week 14: System refinement based on user feedback and final testing.

After the user testing sessions in Week 11–13, I spent the next week refining the system based on all the observations and comments I collected. These weren’t big dramatic changes, but a series of careful adjustments that made the Colour Soundscape feel more stable, calmer, and closer to what I originally imagined.

One of the first things I worked on was smoothing the sound transitions even further. During the tests, I noticed that users were very sensitive to small audio fluctuations—especially the CVD participants—because the sound became a kind of emotional indicator for them. So I added a slightly longer fade-in period and a more forgiving threshold before the sound switched between colours. It made the system feel slower in a good way, almost like it was breathing with the user instead of reacting instantly to every flicker in the camera feed.

I also re-adjusted several sound files. The “red” tone felt a bit too sharp when triggered repeatedly, so I softened the attack and lowered its brightness slightly. The “green” tone received the opposite treatment—just a little more clarity so it didn’t get confused with yellow in borderline conditions. These were small edits, but once I heard them in the context of the arch device, they felt immediately more balanced.

The physical setup also got some minor fixes. I adjusted the camera angle to reduce the reflection spots on shinier fruit, and I added a thin matte paper surface under the plate to minimise glare. These details weren’t obvious during early tests, but they made the colour detection significantly more stable. I also dimmed the LED strip by one level, because a few users mentioned that the light felt “a bit too bright” when they leaned in. The slightly softer light made the whole setup feel more comfortable and less like a technical instrument.

Once these refinements were done, I invited two of the original participants back for a quick round of final testing—one with CVD and one with typical vision. I didn’t guide them too much; I just asked them to repeat the same food-selection tasks. Both of them immediately noticed that the system felt “calmer.” The CVD participant in particular said that the tones “didn’t wobble as much as last time,” which for me was one of the biggest signs that the smoothing adjustments were working. The typical-vision participant said the interaction felt “more natural” and that she wasn’t as distracted by the sound shifts as before.

By the end of Week 14, the system didn’t feel perfect, but it felt coherent—like the different parts (light, sound, camera, interaction) were finally working together instead of competing with each other. What mattered most to me was that the experience felt gentle, not demanding. The goal was never to produce a perfect classification tool, but to create a small ecosystem where sound supports colour in a subtle, reassuring way. These final refinements helped me get closer to that vision.

Looking ahead, I can see so many ways this project could grow—mobile integration, a more adaptive sound engine, or even real-time learning tuned to each user’s perception. But for now, I’m satisfied with where the system landed. It feels honest to the idea I started with: a multisensory companion for moments of uncertainty.

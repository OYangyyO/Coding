# Imagined Eyes: AI and Pareidolia

| ğŸ”¥ | All project files can be found here:| [Google Drive Files for this Project](https://drive.google.com/drive/folders/1fp08pZkzdRHG5Kq375AvJ5nvGtY5Cb0o?usp=sharing) |
| :------------------- | :----------: | ---------- |
| âš¡ | **Within the contents of the README, relevant project files will also be referenced within the text.**  | **Related images can be found in the file.** |
| â¤ | **Three Python notebooks from classroom activities can be found here:**  | [1](https://drive.google.com/drive/folders/1RykP3xuGtoyhSE7LWvn2cie9Ev_FWnGi?usp=sharing)/[2](https://drive.google.com/drive/folders/1zn6F3MSS_Rh3q2jnu8r2UCrN0LpDSZ4X?usp=sharing)/[3](https://drive.google.com/drive/folders/1DN9OE7mfxPISbh6fRB8qwBDAZ_m-JXgU?usp=sharing) |

## Project Introduction
This project stems from my interest in the parallels between the human tendency to â€œsee facesâ€ and the way AI models generate face-like hallucinations. In visual psychology, pareidolia refers to the phenomenon where people perceive familiar patterns, especially faces, in random stimuli. For example, we may see human-like features in power sockets, rice cookers, or the arrangement of windows, a tendency rooted in our evolutionary preference for social recognition[^1].

Part of the inspiration came from [Google Faces](https://onformative.com/work/google-faces/) by onformativeâ€”a visual art project where an AI system was trained to â€œhallucinateâ€ faces from Google Street View scenes. Using CLIP, it identified face-like structures in mundane street imagery.[^2] The project made me wonder: when AI claims to â€œsee a face,â€ is it making an error, or is it unintentionally mimicking our own perceptual projections? Could we systematically reconstruct this hallucination mechanism through a technical pipeline?

![googlefaces_01](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/25e63b06-0534-468e-bf1c-5a7afad3d79d)
Google Faces, searching for faces on earth, 2013 [^3]

Taking this project as a starting point, I wanted to reverse the question: **what if these hallucinations are not human misperceptions, but rather moments when AI truly sees something? When a deep learning model mistakenly classifies an image as a face, with no human intervention in labelling or annotation, what exactly is it recognising? What does it "see"? And more intriguingly, if we ask the AI to explain its perception in words, what kind of narrative will it construct?**

In this project, I built a hallucination-generation system that combines image classification, Grad-CAM visualization, text generation, and CLIP-based image-text similarity scoring. The system takes in images from three categories (Real Face / Pareidolia / Non-Face), classifies them, and then automatically generates a sentence that describes the image from the AIâ€™s perspective. These sentences are not pre-written templates, but are generated by GPT-2 based on visual keywords extracted from Grad-CAM activation maps, and then selected using CLIP to find the most semantically aligned match. The final output is a triptych: the original image, its corresponding attention heatmap, and an AI-generated "hallucinated" descriptionâ€”together forming a visual-linguistic card.

This project acts as a cognitive experiment to explore a larger question: **When artificial intelligence misidentifies random patterns as faces, is this behavior a recognition error, or is it imitating the human imagination mechanism?** In *The Promethean Dilemma of AI at the Intersection of Hallucination and Creativity* (Chakraborty & Masud, 2024), the authors argue that hallucinations in generative AI, while often considered failure modes, may resemble cognitive confabulations: artefacts of imagination under uncertainty. They pose the provocative question: To what extent can the hallucinations arising from GenAI models be considered creative?[^4]

By encouraging GPT-2 to generate justifications for why certain textures appear facial, this project reframes hallucinations not as flaws, but as emergent semiotic behaviours. The results arenâ€™t always correct, but they are often poetic, sometimes uncanny, and occasionally profound in their resemblance to the way humans assign meaning to ambiguity.

As a student in creative computing, my goal through this project is to explore two questions: How does AI see differently from us? And when it tries to explain what it sees, are we truly listening to its hallucinations? This system serves as both a research tool and a speculative lensâ€”a hybrid experiment in perception, narration, and visual semiotics. It attempts to visualise machine subjectivity, and in doing so, holds a mirror to our own.

## Approach & Process
This project is based on a three-stage image-language processing pipeline, aiming to study the mechanism of AI hallucination in facial recognition tasks. The entire pipeline is completed completely locally offline, combining image classification models, Grad-CAM visualization, text generation (GPT2), and semantic scoring (CLIP).
### First step: Image Classificationï¼ˆreal / pareidolia / non-faceï¼‰ ğŸ“·
#### Project structure (main files)ï¼š
- [[Dataset](https://drive.google.com/drive/folders/10tg-rwcwwHWZii3Q06G6k8U4skAma5ks?usp=sharing)]
- [[Training set1](https://drive.google.com/file/d/1c1qLlytqjlrDiNis3lPrURlCG9OHGM8J/view?usp=sharing)/[Training set2](https://drive.google.com/file/d/1I9YfN7S_0TfJcSIPkow1hbK5BLSKVy3u/view?usp=sharing)]
- [[The model1](https://drive.google.com/file/d/1W6gqDkGjpJHySkM-Eq3HNzbaiw1-5q00/view?usp=sharing)/[The model2](https://drive.google.com/file/d/1JJiLwff3kptQXY-2tMr4I7pdvAY26g1l/view?usp=drive_link)]
- [[Test image classification1](https://drive.google.com/file/d/1OuJSK24sDig0BnF3x3sRgsb6bk7u-4y4/view?usp=sharing)/[Test image classification2](https://drive.google.com/file/d/1cJ7mkDnTqesnZxjdotrMzDy4jGYnhu0s/view?usp=sharing)]
#### Models used:
- [ResNet18 (pre-trained)](https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html)
- [EfficientNet-B0 (using efficientnet_pytorch)](https://github.com/lukemelas/EfficientNet-PyTorch#loading-a-pretrained-model)
- Class 4 Notebook

At the beginning of the project, I drew on the notes from the fourth class and tried to build a three-class classification model to identify whether an image contains:
+ Real Face
+ Pareidolia
+ Non-Face

**To verify the model's ability to recognize pareidolia, I used two versions of the model for training and comparison:**
+ In `Train.ipynb`, I used `ResNet18` as the base model and trained and validated it in the dataset constructed in ImageFolder. The model output is saved as `pareidolia_model.pth`.
+ In `Train2.ipynb`, I used `EfficientNet-B0` instead, preprocessed the input images with normalization (using ImageNet mean and std), trained them and saved them as `efficientnet_model.pth`.

**Training effect:**
+ The `EfficientNet-B0` model converged quickly within 10 epochs, with a verification accuracy of over 90%, and performed more stably than `ResNet18`.
+ All training is done on local CPU (autodetect `cuda`).

**Testing and Visualization:**
+ I use the `Test_False.ipynb` and `Test_update.ipynb` notebooks to test the two models and visualize some of their predictions (showing the true labels and model predictions for three random images each time).
+ Precision is presented via a bar graph, with the prediction accuracy noted as a percentage.
  
![2](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/56c51e2c-eaa0-4ef1-9209-2b4618469006)
![2-1](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/55a89c3b-0a92-4583-bd18-a971c11beaeb)

After completing the training and testing of the classification model, I noticed that the model still judged some "non-face" images as "faces". At first I thought it was a data quality issue, but as more samples were added, I began to wonder whether the model's "misjudgments" were really just errors, rather than some kind of "illusions" with human perception characteristics. These prediction biases seem to be concentrated on certain local structures, such as the holes in the sockets, the folds of the curtains, the bifurcations of the tree trunks, etc., with a certain "eye-mouth" arrangement.


### Second step: Thermal imaging and â€œAI attentionâ€ assessmentğŸ“š
Use the model:
- [Grad-CAM visualization tool (adapted to EfficientNet)](https://github.com/jacobgil/pytorch-grad-cam)
- [OpenAIâ€™s CLIP (ViT-B/32)](https://github.com/openai/CLIP)
  
In order to more intuitively understand the "attention" of AI when classifying images, I decided to introduce the Grad-CAM method to visualize the activation hotspots of the model. By superimposing the heat map on the original image, I hope to find a way to not only confirm what the model "saw wrong", but also try to answer: "What did AI see?"

This decision made me shift from the pursuit of simple classification accuracy to the exploration of the model perception mechanism, and also provided a visual semantic basis for keyword extraction and prompt word construction in the subsequent "hallucination text generation" module.

#### Two major steps were carried out in this phase:
**1. Grad-CAM Heatmap Generation**

Using the [Grad-CAM module](https://github.com/jacobgil/pytorch-grad-cam), I computed attention overlays on input images. These visualizations highlighted the areas the model relied on most when making its classification decisions. In some cases, these hotspots corresponded to object regions that resembled eyes, mouths, or facial symmetryâ€”a sign that the model might be responding to similar cues as humans during pareidolia perception.

Files used:
+ [Grad-CAM Heatmap Generation1](https://drive.google.com/file/d/1R1cGMykWSIXni9mZeJuiOnnz_m3WmmIg/view?usp=drive_link)
+ [Grad-CAM Heatmap Generation2](https://drive.google.com/file/d/188w1ceMqYok7OsmMiEgCZ9ABhkQ8KfZ7/view?usp=drive_link)
  
![6](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/c36c654e-4d45-4fa7-a303-191d7fe6ba0a)
![7](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/104ff397-106a-495a-898e-a0634dee510d)
![8](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/acae2c41-5566-488a-8702-7717a559ca0d)

  
**2. CLIP Similarity Distribution Analysis**

To add a semantic layer to attention evaluation, I also used CLIP to compute the similarity between the image and several textual prompts (e.g., â€œa human faceâ€, â€œa face-like objectâ€, â€œnothing like a faceâ€). This step helped assess how close the image was, conceptually, to face-related categories according to a pretrained vision-language model.

I plotted similarity distributions across the three categories to see if pareidolia samples clustered differently from real or random images. Interestingly, while real faces consistently had high â€œfaceâ€ similarity, some pareidolia images scored similarlyâ€”suggesting that these illusions aren't just visual artifacts, but semantically aligned with the concept of a face.

Files used:
+ [analyze_clip_results](https://drive.google.com/file/d/143s3xMrsUWWs09B_hXI4r36qu5aRnGHo/view?usp=drive_link)
+ [CLIP score distribution analysis](https://drive.google.com/file/d/1KMXPvsZeYAbK2qhG270dwux6cSPVHlFt/view?usp=drive_link)

I used CLIP to score all images with two sets of text prompts: "a human face" and "a face-like object". The results show that **the scores of Pareidolia images are significantly higher than those of Real Face and Non Face** (see the figure below), further revealing that the model responds strongly to "hallucinations" at the semantic level.

![3](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/f248284e-53bc-42c7-b13f-ed435d2e89bb)
![4](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/c7219248-2375-4b9a-be5d-5e4741a718c9)

To test whether CLIP also exhibits the same bias at the label level, I constructed a confusion matrix. The results show that **47 of the non-face images were misclassified as Pareidolia**, which means that even when the image itself has no obvious facial features, the model still tends to "complete" it into a face-like structure (as shown in the figure below).

![5](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/f0c62cf4-a413-42a1-925b-d8a5ecd1f25d)

This stage verified my intuition: AI has a tendency to "overfit" human-like structures, which is not only reflected in the distribution of image hot spots, but also in the "overmatching" behavior of the semantic layer and the label layer. This discovery provided a theoretical basis for me to enter the third stage - generating "AI hallucination explanation text": AI not only "sees hallucinations", it also "defends" hallucinations.

### Inter-stage failure case: trying to use Pareidolia for image regeneration ğŸ¤¯
Try the model:
+ [Stable Diffusion 1.5](https://github.com/CompVis/stable-diffusion)
+ [CLIP-guided Diffusion](https://github.com/crowsonkb/guided-diffusion)
  
Before I implemented AI hallucination text generation, I tried to input the recognized Pareidolia image into the `img2img` module of [Stable Diffusion](https://github.com/CompVis/stable-diffusion) to try to generate a "more human-like" face image.
My original intention was to let AI not only recognize Pareidolia, but also "enhance" it and make the hallucination concrete. Let it "draw" what it thinks is anthropomorphic.
However, in actual operation, the following problems occurred:
- When using `img2img`, it is difficult to control the model to evolve stably towards the face, and the generated images often present strange shapes or directly blur the original structure.
- Attempts to optimize the strategy guided by CLIP failed (little improvement in similarity and no convergence in image semantics). And it crashed my computer.

The experimental results show that this strategy is difficult to achieve stable control. The generated images are often distorted, with facial features severely stretched or disorganized. The model tends to over-anthropomorphize local structures, and although the final image has the shape of a "face", it does not present the realistic human face I expected. The figure below is a typical failure example - although it retains some elements of the "smiley face" expression, the protrusions and structural proportions of the nose are completely deviated from the structure of a real human face. Most importantly, it is difficult to support my argument.

Failure Casesï¼š

![d9c1269a9e20efdc703b8157203f6071](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/4dfca2e2-cae6-4ad7-a2a5-645d982a03b8)
![img2img_pareidolia_face](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/5051fdf7-13c7-4293-9dc1-e11c08166d8f)

  
This failed attempt made me realize that the occurrence of hallucinations is not equivalent to the concrete restoration of the face. AI does not actively "complete" the facial features in blurry images like humans do. Instead, it generates forms that deviate from the target based on local similarities based on the image features learned in training. I gave up this generation path and began to think about whether I could use words to express "AI's hallucination of images." This pushed me to refocus on the direction of "how AI explains hallucinations" and became the key motivation for me to enter the third stage!

### Third step: AI Hallucination Text Generation (GPT2 + CLIP)ğŸ’¬
Use the model:
- [GPT-2 large](https://huggingface.co/openai-community/gpt2-large)(Text Generation)
- [Grad-CAM](https://github.com/jacobgil/pytorch-grad-cam)(for keyword extraction)
- [CLIP by OpenAI](https://github.com/openai/CLIP)

After completing image classification and attention visualization, and after trying to regenerate images without success, I raised a new question: **If AI is asked to explain "it saw a face", how will it describe the hallucination?**

Based on this curiosity, I began to build a **visual-language generation system**, trying to give "reasons" to the model's misjudgment behavior, not only analyzing what it saw, but more importantly, letting it "say" why it had hallucinations.

This stage is not only a challenge to technical capabilities, but also an exploration of the relationship between AI perception and expression. 
#### In this process, I gradually built a multimodal hallucination explanation generation process:
**1. Keyword extraction and prompt construction**

Using the Grad-CAM heatmap from the previous stage, I extracted several "facial structure clue keywords" from each image, such as `left eye`, `mouth-like curve`, `facial symmetry`, etc. These words can be organized into an explanatory prompt, such as:
> *"It looks like a face because it has left eye, right eye, and a mouth-like curve."*
> 
This step builds a vision-to-language translation bridge, making subsequent text generation more targeted and controllable.

Files used:
- [[main](https://drive.google.com/file/d/1g-of9bJgocUMIZPSRUJbtdYPr6uX5T8G/view?usp=drive_link)](Overall process main script)
- [[Grad-CAM_update](https://drive.google.com/file/d/188w1ceMqYok7OsmMiEgCZ9ABhkQ8KfZ7/view?usp=drive_link)](Heatmap and keyword extraction)
  
**2. Hallucination Explanation Text Generation**

Based on the above prompt, I used the `GPT-2 large` model to generate 5 candidate texts, representing AI's various possible interpretations of this image. These sentences do not use templates, but are freely generated by the language model. The style may be scientific, poetic, or even absurd - but they all try to explain "why this looks like a face" from the perspective of AI.

Files used:
- [[main](https://drive.google.com/file/d/1g-of9bJgocUMIZPSRUJbtdYPr6uX5T8G/view?usp=drive_link)](Text generation logic)
- [[Grad-CAM_update](https://drive.google.com/drive/folders/198_HfCH0QkCNr3xtQ_bqxK4qZImhxKoZ)](Store all generated candidate sentences)
  
**3. CLIP similarity filtering and text selection**

To avoid irrelevant redundant words in the generated sentences, I used CLIP to score the semantic similarity between each generated sentence and the original image, and selected the sentence with the highest score as the final output. This step ensures that the final generated content is semantically highly consistent with the image and avoids the problem of "semantic drift".

Files used:
- [[Text-Matching](https://drive.google.com/file/d/10QQhzaYda7ZXXFoi_mvHfc_c3786sLV1/view?usp=drive_link)](CLIP semantic matching score)
- [[clip_val_with_hallucinations](https://drive.google.com/file/d/1zA3EgvSDGLwZdYS4F0XqM-5XGWrK3R6y/view?usp=drive_link)](Final score analysis)

#### Example Resultsï¼š
![comparison_card_1](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/e842359c-b924-406c-ae72-5b1b5665b81a)
![comparison_card_6](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/c4d1a1cf-f634-471d-898d-f2c5be86c8a3)
![comparison_card_5](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/c8029655-e347-40cd-82f4-2c77cddda443)
![comparison_card_4](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/5d9f2ab8-decf-493e-88c0-5378fe5d02da)
![comparison_card_3](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/c1fd513c-01bc-4418-a22e-ced8567f32ec)
![comparison_card_2](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/9e24a26f-8f50-48c5-9193-d802bda854e5)

In the final generation display of this stage, there is one case that is particularly worth analyzing in depth: in the third set of cards, when the model encounters a yellow suitcase (Pareidolia class), it not only focuses significantly on the â€œeyesâ€ and â€œmouthâ€ structure composed of the â€˜handleâ€™ and â€œlatchâ€ in the Grad-CAM visualization, but also describes it as follows in the hallucination text:

> â€œA structure resembling a human face, with a few eyes. The model interpreted the appearance and behavior of an individual face as an image of a face. But how, exactly, are faces presented in the world?â€

This description is semantically vague, yet it unexpectedly approaches a highly philosophical proposition: **does a face exist in the object itself, or is it constructed by the observer's cognitive projection?**

In this segment, GPT-2 is guided to generate not just object attributes or scene information, but an attempt to reason about â€œwhat makes a face a face.â€ This result demonstrates that when I feed the attention keywords extracted from the Grad-CAM heatmap into the text generation model, the system does not merely mechanically repeat the image content, but simulates a behavior akin to a â€œself-explanatory illusion.â€

In contrast, consider another sample from the Non-Face category: the scene depicts an extremely ordinary indoor corner, with the model's attention hotspots scattered and almost no facial features present. However, the text generation produces the following description:

> â€œThe result was that the AI went on to predict the outcome of the womanâ€™s actions, without having heard her speak. Why is this plausible?â€

Although this passage contains logical leaps, it nonetheless constitutes a shocking example of â€œanthropomorphicâ€ hallucinatory reasoningâ€”AI not only generated a female character but also attempted to â€œguessâ€ her psychological state and behavioral intentions. This further demonstrates that hallucinations are not merely simple recognition biases but may instead represent a semi-structured, subjectively informed cognitive output.

Through these visual-language outputs, we can see that AI hallucinations are not only visible and interpretable but may also be â€œnarratively articulatedâ€ to some extent. This narration is no longer a reaction based on real images but simulates the human process of linguistically reconstructing hallucination phenomena, prompting us to re-examine the essence of hallucinations: they are not purely deviations but may also represent generative cognitive activities.

Therefore, this phase of the experiment does not stop at visualizing the model's â€œerrors,â€ but attempts to translate the errors themselves into a new narrative mechanism. In this process, AI transforms from an identifier to a narrator, allowing us to glimpse the semi-human form of machine hallucinations.



## Results & Reflections
### Results presented
The six â€œhallucination cardsâ€ ultimately generated clearly integrate the system's output into a unified format comprising images, Grad-CAM heatmaps, and AI text descriptions, thereby presenting the â€œhallucination generation resultsâ€ of different image types (Real Face / Pareidolia / Non-Face) as perceived by the AI. From an image structural perspective, the model successfully identified real human faces and explicitly stated â€œno hallucinationsâ€; in pareidolia images, the model exhibited a tendency toward anthropomorphic associations, such as describing them as â€œblurred human-like structures,â€ â€œa standing figure,â€ or â€œa familiar faceâ€; in Non-Face images, the generated text exhibited greater abstraction or semantic drift, such as â€œThis resembles some kind of flying objectâ€ or â€œI witnessed the actions of a woman.â€ These texts reflect the model's attempt to forcefully interpret unrelated images as human-related.

The outputs from this stage allowed me to achieve my initial goal: simulating how AI â€œmisidentifies facesâ€ and generates seemingly reasonable narratives. Although these sentences are not real, their structure and descriptions are surprisingly â€œreasonable,â€ aligning with the perspective in the paper *â€œThe Promethean Dilemma of AI at the Intersection of Hallucination and Creativityâ€ (Chakraborty & Masud, 2024)*â€” **â€œAI hallucinations may be a semi-rational creative behavior emerging from uncertainty.â€**[^5] From a certain perspective, they resemble the cognitive filling-in humans perform when interpreting ambiguous scenarios.

### Visual explanation and conclusion verification
To ensure that the system not only generates content that appears reasonable but also captures the â€œattentionâ€ of the AI toward specific regions of interest in images, I employed a ResNet50 architecture combined with Grad-CAM in the second phase to extract heatmaps. Compared to more complex architectures like ViT or Swin Transformer, ResNet50 possesses stronger spatial convolution-based inductive biases, enabling it to more clearly highlight the CNN's response priorities in â€œlocal structures.â€ Considering that pareidolia often occurs in locally symmetric and boundary-focused regions, this choice makes the attention map more readable and valuable for comparison.

![å±å¹•æˆªå›¾ 2025-06-16 164249](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/ad286f1b-36e7-435c-a681-54f4dbe9b469)[^6]

I also combined the CLIP image-text similarity verification mechanism to automatically screen the semantic content of each generated text. The key objective of this stage was to understand where the AI was â€œfocusingâ€ and whether it had truly â€œseenâ€ facial features or merely captured similar structures.

For example, in the images generated by Grad-CAM, I observed that in Pareidolia images (such as sockets, pot lids, and suitcases), the activated regions were primarily concentrated in areas characterized by â€œsymmetry,â€ â€œdark concentrated regions,â€ or â€œapproximate eye and mouth structures,â€ which aligns closely with the â€œfalse facesâ€ perceived by humans in the pareidolia phenomenon; in Non-Face images, these attention regions exhibit random activation states such as â€œedge diffusionâ€ or â€œnon-core object areas.â€ This contrast indicates that even if the AI mistakenly identifies certain Non-Face images as â€œfaces,â€ its judgment is based on vague and random criteria.

![10](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/963e97bb-89e0-4419-a6aa-a1e6b8a4c2da)
![11](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/d3bddf00-99e4-4d43-9027-3934d3c947f8)
![12](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/assets/1301/7a292b75-7bad-4387-9df1-41b4c14c4431)

To further verify whether the text is related to the image, I used CLIP's text-image similarity for multiple rounds of matching, selecting sentences most semantically aligned with the original image. Through score distribution plots (such as [violin plots](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/blob/main/Violin%20diagram.png) and [box plots](https://git.arts.ac.uk/24010647/Coding3-Xiwen-Yang/blob/main/box%20plot.png)), I found that the score distributions for pareidolia images are generally higher than those for Non-Face images, and in some cases even exceed those for Real Face images. This may suggest that AI's sensitivity to illusions is semantically no less than that of â€œnormal recognition.â€

These findings support my core argument: AI is not merely making mistakes but constructing a â€œmachine fantasy.â€ The illusions it perceives in the world may not be any fewer than ours.

### Project Reflection and the Role of AI
As a student, I was initially just curious: How would AI â€œdescribe imagesâ€? However, after actually implementing this system, I gradually realized that AI's role in it goes far beyond simply executing commands; it is more like a biased, unreliable, yet unexpectedly creative co-author.

On one hand, I had to provide input, select models, and define hotspot keywords, meaning the generation process was not entirely autonomous but involved repeated experimentation through multiple iterations in data preparation, parameter settings, and prompt design. On the other hand, when I intentionally refrained from intervening in the text generation template, GPT-2 produced descriptions with highly â€œhallucinatoryâ€ characteristics. These non-standard sentences, though grammatically chaotic, imply a vague, poetic, dreamlike linguistic logic, offering me ample room for reflection.

This process has led me to rethink the boundaries of AI creativityâ€”if image recognition and hotspot localization represent â€œAI seeing,â€ then hallucinatory text generation represents â€œAI speaking.â€ When these texts are not predefined templates but are automatically generated and semantically filtered, can we say that AI is engaging in a spontaneous â€œhallucinatory narrationâ€? What I see here is not merely algorithmic recognition results but a boundary zone between â€œcognitive illusionsâ€ and â€œimaginary texts.â€

## AI tool disclosure
- No AI tools were used to generate or assist with the coding process. All model training, data handling, and implementation were completed manually.
- I used GPT-4o to help search for and interpret model plugin documentation, especially during model selection, parameter tuning, and error troubleshooting. The decision-making and integration processes were carried out independently, with GPT-4o acting as a supplementary reference rather than a generative assistant.

## Bibliography

[^1]: Pareidolia (2025) Wikipedia. Available at: https://en.wikipedia.org/wiki/Pareidolia?utm_source=chatgpt.com (Accessed: 10 June 2025). 
[^2]: Google faces (2013) onformative. Available at: https://onformative.com/work/google-faces/ (Accessed: 10 June 2025). 
[^3]: Google faces (2013) onformative. Available at: https://onformative.com/work/google-faces/ (Accessed: 12 June 2025). 
[^4]: Tanmoy Chakraborty and Sarah Masud. 2024. The Promethean Dilemma of AI at the Intersection of Hallucination and Creativity. Commun. ACM 67, 10 (October 2024), 26â€“28. Available at: https://doi.org/10.1145/3652102 (Accessed: 9 June 2025). 
[^5]: Tanmoy Chakraborty and Sarah Masud. 2024. The Promethean Dilemma of AI at the Intersection of Hallucination and Creativity. Commun. ACM 67, 10 (October 2024), 26â€“28. Available at: https://doi.org/10.1145/3652102 (Accessed: 14 June 2025). 
[^6]: Jacobgil (no date) Jacobgil/Pytorch-Grad-cam: Advanced AI explainability for Computer Vision. support for CNNS, vision transformers, classification, object detection, segmentation, image similarity and more., GitHub. Available at: https://github.com/jacobgil/pytorch-grad-cam (Accessed: 16 June 2025). 
